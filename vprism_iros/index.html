<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Herbert Wright1, Weiming Zhi2, Matthew Johnson-Roberson2, Tucker Hermans1,3 1University of Utah, 2Carnegie Mellon University, 3NVIDIA">
  <title>V-PRISM: Probabilistic Mapping of Unknown Tabletop Scenes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/white.css" id="theme">
  <link rel="stylesheet" href="styles.css"/>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">V-PRISM: Probabilistic Mapping of Unknown Tabletop
Scenes</h1>
  <p class="author"><span class="med-text">Herbert Wright<sup>1</sup>,
Weiming Zhi<sup>2</sup>, Matthew Johnson-Roberson<sup>2</sup>, Tucker
Hermans<sup>1,3</sup></span><br />
<span class="small-text"><sup>1</sup>University of Utah,
<sup>2</sup>Carnegie Mellon University, <sup>3</sup>NVIDIA</span></p>
  <p class="date"><img src="fig1_updated.png" class="title-img"/></p>
</section>

<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<ul>
<li>Many algorithms for manipulation need a 3D representation of a
scene</li>
<li>Most existing reconstruction methods don’t contain <em>principled
uncertainty information</em></li>
<li>Most existing methods also don’t take <em>multiple objects</em> into
account</li>
</ul>
<p><img data-src="./motivation.png" style="width:50.0%" /></p>
</section>
<section id="contributions" class="slide level2">
<h2>Contributions</h2>
<ul>
<li>Formulate reconstruction as multi-class mapping problem</li>
<li>A <em>Bayesian</em> framework for building <em>continuous</em>
segmentation maps of tabletop scenes</li>
<li>An object-centric sampling method to encode depth information</li>
</ul>
<p><img data-src="sigmoidbhmfails.png" style="width:76.0%" /></p>
</section>
<section id="background-hilbert-maps" class="slide level2">
<h2>Background: Hilbert Maps</h2>
<div class="columns">
<div class="column" style="width:40%;">
<figure>
<img data-src="./hilbertmaps.png"
alt="Figure from (Ramos and Ott 2016), Input data on Left, Hilbert map on Right, SVM in Middle" />
<figcaption aria-hidden="true">Figure from <span class="citation"
data-cites="ramos2016hilbert">(Ramos and Ott 2016)</span>, Input data on
<strong>Left</strong>, Hilbert map on <strong>Right</strong>, SVM in
<strong>Middle</strong></figcaption>
</figure>
<figure>
<img data-src="./hingepoints.png" style="width:85.0%"
alt="Example of hingepoints" />
<figcaption aria-hidden="true">Example of hingepoints</figcaption>
</figure>
</div><div class="column" style="width:60%;">
<ul>
<li>Hilbert Maps, introduced in <span class="citation"
data-cites="ramos2016hilbert">(Ramos and Ott 2016)</span>, are a
technique for mapping an environment via depth measurements</li>
<li>First, a dataset is created with terminal points of each ray and
sampled negative (unoccupied) points</li>
<li>Next, a set of hinge points are constructed in a grid</li>
<li>Then, regularized logistic regression is run in order to find
weights <span class="math inline">w</span> for a map <span
class="math inline">m</span> with occupancy probability: <span
class="math display"> m(x) = \sigma (w^\top \phi(x)) </span> where <span
class="math inline">\phi</span> is a feature transform constructed from
the hinge points</li>
</ul>
</div>
</div>
</section>
<section id="background-bayesian-hilbert-maps" class="slide level2">
<h2>Background: Bayesian Hilbert Maps</h2>
<ul>
<li>Hilbert Maps were given a Bayesian treatment in <span
class="citation" data-cites="senanayake2017bayesian">(Senanayake and
Ramos 2017)</span></li>
<li>Weights <span class="math inline">w</span> were modeled as a
Gaussian <span class="math inline">w \sim \mathcal N(\mu,
\Sigma)</span></li>
<li>Variational inference as described in <span class="citation"
data-cites="jaakkola1997variational">(Jaakkola and Jordan 1997)</span>
was performed to recover a Posterior distribution over the weights,
<span class="math inline">\mathcal N(\hat \mu, \hat \Sigma)</span>. This
involved an iterative EM algorithm.</li>
<li>Predictions were made by approximating <span
class="math inline">\mathbb E_w [\sigma (w^\top \phi(x))]</span></li>
<li>Because of the Bayesian treatment, the method exhibits principled
uncertainty</li>
</ul>
<figure>
<img data-src="./bhm.png"
alt="Figure from (Senanayake and Ramos 2017)" />
<figcaption aria-hidden="true">Figure from <span class="citation"
data-cites="senanayake2017bayesian">(Senanayake and Ramos
2017)</span></figcaption>
</figure>
</section>
<section id="problem-setup" class="slide level2">
<h2>Problem Setup</h2>
<ul>
<li>We phrase the scene reconstruction problem as a mapping problem</li>
<li>Given a depth observation (point cloud) of a scene and a correct
instance segmentation, construct a map that maps points in 3D space to
one of the segmentations (or empty space)</li>
<li>This is a multiclass segmentation/mapping problem, because we want
to avoid interpenetration between the reconstructions (see image
below)</li>
<li>We assume we know our camera position (relative to point cloud) and
that points in between the camera and point cloud are unoccupied</li>
<li>We assume each object lies on or above a planar surface such as a
tabletop</li>
</ul>
<p><img data-src="./sigmoidbhmfails.png" style="width:80.0%" /></p>
</section>
<section id="method-overview" class="slide level2">
<h2>Method Overview</h2>
<p><img data-src="method_overview.png" /></p>
<ul>
<li>We want to adapt a similar strategy as Bayesian Hilbert Maps <span
class="citation" data-cites="senanayake2017bayesian">(Senanayake and
Ramos 2017)</span> for our task</li>
<li>First, we perform negative sampling, but adapt it to an
object-centric mapping task</li>
<li>Then we run an EM algorithm with our constructed dataset to create a
probabilistic multiclass map
<ul>
<li>This means we will need to adapt the original EM algorithm from
<span class="citation" data-cites="jaakkola1997variational">(Jaakkola
and Jordan 1997)</span> to extend to a softmax activation function</li>
</ul></li>
</ul>
</section>
<section id="negative-sampling" class="slide level2">
<h2>Negative Sampling</h2>
<ul>
<li>Points at end of the ray are labeled as occupied</li>
<li>Randomly sample points along the rays that are specifically
<em>nearby the objects</em></li>
<li>Find the table with RANSAC and sample points underneath it
<em>nearby the objects</em></li>
<li>Run grid subsampling from <span class="citation"
data-cites="thomas2019learning">(Thomas 2019)</span> in order to reduce
the total amount of points and make the data more uniform</li>
</ul>
<p><img data-src="./placeholder_negative_sampling.png"
style="width:80.0%" /></p>
</section>
<section id="softmax-em-algorithm-beyond-sigmoid-hilbert-maps"
class="slide level2">
<h2>Softmax EM Algorithm: Beyond Sigmoid Hilbert Maps</h2>
<ul>
<li>Binary Sigmoid maps from <span class="citation"
data-cites="senanayake2017bayesian">(Senanayake and Ramos 2017)</span>
require serious adjustments to extend to a multiclass version.</li>
<li>Sigmoid activation needs to be updated to Softmax, and weights is
now a matrix instead of a vector.</li>
<li>The bounds and corresponding solutions used for the EM algorithm in
<span class="citation" data-cites="jaakkola1997variational">(Jaakkola
and Jordan 1997)</span> cannot be directly applied to the Softmax
function; a new bound and corresponding EM algorithm is required.</li>
<li>Similarly, the approximation used for efficiently predicting new
points doesn’t work for the Softmax function; a new approximation for
the expectation must be used.</li>
</ul>
</section>
<section id="softmax-em-algorithm-training" class="slide level2">
<h2>Softmax EM Algorithm: Training</h2>
<div class="columns">
<div class="column" style="width:55%;">
<ul>
<li>We model weights as a matrix <span class="math inline">W</span>
where each row is normally distributed: <span class="math inline">W_k
\sim \mathcal N (\mu_k, \Sigma_k)</span></li>
<li>Using math from <span class="citation"
data-cites="bouchard2007efficient">(Bouchard 2007)</span>, which builds
off of <span class="citation"
data-cites="jaakkola1997variational">(Jaakkola and Jordan 1997)</span>,
We can develop an EM algorithm:</li>
<li>First, we can define a surrogate lower bound likelihood: <span
class="math inline">Q(y = y_i | W, x_i; \alpha, \xi)</span>, introducing
two variational parameters <span class="math inline">\alpha,
\xi</span></li>
<li>Then, we can iterate between:
<ul>
<li>solving for a more accurate surrogate</li>
<li>updating posterior distribution with more accurate surrogate</li>
</ul></li>
</ul>
</div><div class="column" style="width:45%;">
<p><img data-src="./algorithm.png" style="width:100.0%" /></p>
</div>
</div>
</section>
<section id="inference-of-softmax-map" class="slide level2">
<h2>Inference of Softmax Map</h2>
<ul>
<li>In order to predict new points we need to evaluate: <span
class="math display"> \hat P(y = k | x) = \mathbb E_W \left[
\text{softmax} (W \phi(x)) \right]_k </span></li>
<li>This doesn’t have a closed form solution, but <span class="citation"
data-cites="daunizeau2017semi">(Daunizeau 2017)</span> outlines how you
can approximate it with the equality: <span class="math display">
\text{softmax}(  a)_k = \frac{1}{2 - c + \sum_{i \neq k} \sigma (  a_k
-   a_i)^{-1}}, </span></li>
<li>Using this approximation, combined with the one used for Bayesian
Hilbert Maps, we get: <span class="math display"> \mathbb E_{  W} \left[
\text{softmax} ({  W} \phi(  x)) \right]_k \approx \frac{1}{2 - c +
\sum_{i \neq k} \mathbb E [\sigma (\tilde{  z}_i)]^{-1}} </span> where
<span class="math inline">\tilde{  z}_i = [  W \phi(  x)]_k -   [  W
\phi (  x)]_i</span>.</li>
</ul>
</section>
<section id="experiments-overview" class="slide level2">
<h2>Experiments Overview</h2>
<ul>
<li>We want to answer the following questions via experiments:
<ul>
<li>Can we accurately reconstruct completely unknown objects?</li>
<li>Does our new negative sampling method improve reconstruction
quality?</li>
<li>Is our method robust to noisy scenes?</li>
<li>Does our map capture understandable and principled uncertainty about
the scene?</li>
</ul></li>
<li>We want to experiment both in simulation and in the real world.
<ul>
<li>Simulated scenes are created using different object datasets <span
class="citation"
data-cites="calli2015ycb chang2015shapenet deitke2023objaverse">(Calli
et al. 2015; Chang et al. 2015; Deitke et al. 2023)</span></li>
<li>Segmentations for real world scenes are computed using Segment
Anything</li>
</ul></li>
<li>Implemented in PyTorch, Code available at: <a
href="https://github.com/Herb-Wright/v-prism">https://github.com/Herb-Wright/v-prism</a></li>
</ul>
</section>
<section id="reconstruction-quality" class="slide level2">
<h2>Reconstruction Quality</h2>
<p><img data-src="table.png" /></p>
<div class="columns">
<div class="column" style="width:25%;">
<p><img data-src="rgb1.jpg" /></p>
<p><span class="med-text">Example Scene</span></p>
</div><div class="column" style="width:25%;">
<p><img data-src="rgb.jpg" /></p>
<p><span class="med-text">Example Scene</span></p>
</div><div class="column" style="width:25%;">
<p><img data-src="view1.png" style="width:80.0%" /> <span
class="med-text">Example Reconstruction</span></p>
</div><div class="column" style="width:25%;">
<p><img data-src="./view2.png" style="width:80.0%" /> <span
class="med-text">Another View of Reconstruction</span></p>
</div>
</div>
</section>
<section id="negative-sampling-1" class="slide level2">
<h2>Negative Sampling</h2>
<ul>
<li>Ablation results of our negative sampling method in simulation
suggest our negative sampling method is superior to alternatives:</li>
</ul>
<p><img data-src="./ablation_table.png" /></p>
</section>
<section id="robustness" class="slide level2">
<h2>Robustness</h2>
<p><em>Here is a qualitative example of reconstructing a noisy real
world scene:</em></p>
<div class="columns">
<div class="column">
<p>Point cloud <img data-src="out2.gif" /></p>
</div><div class="column">
<p>Reconstruction <img data-src="out1.gif" /></p>
</div>
</div>
</section>
<section id="robustness-1" class="slide level2">
<h2>Robustness</h2>
<p><em>Here is a comparison with PointSDF <span class="citation"
data-cites="van2020learning">(Van der Merwe et al.
2020)</span>:</em></p>
<p><img data-src="recon_vs_pointsdf_with_labels.png"
style="width:75.0%" /></p>
</section>
<section id="uncertainty" class="slide level2">
<h2>Uncertainty</h2>
<p><img data-src="uncertainty_fig_with_labels.png"
style="width:78.0%" /></p>
<ul>
<li>We compare uncertainty of V-PRISM <span class="citation"
data-cites="wright2024v">(Wright et al. 2024)</span> with a
non-probabilistic alternative</li>
<li>Uncertainty is high in heavily occluded areas of the scene in our
method, but not the alternative’s</li>
</ul>
</section>
<section id="future-work" class="slide level2">
<h2>Future Work</h2>
<!-- - Right now, the reconstructions are not always physically stable (see image below). 
- In the future I want to work on enforcing a stability *prior* on reconstructed meshes.
- Instead of a heuristic loss function like [@agnew2021amodal], I want to somehow incorporate differentiable simulation to construct a differentiable loss directly on stability
- I also want to utilize V-PRISM maps to inform grasping in a feedback loop.

![](./out_drake.gif){width=30%} -->
<ul>
<li>Enforce object level prior
<ul>
<li>Informative prior without over-regularizing out of distribution
objects</li>
</ul></li>
<li>Uncertainty-awareness in downstream tasks like grasping in
clutters</li>
<li>Use uncertainty for active learning (wrist-mounted camera,
tactile)</li>
</ul>
<p><img data-src="./airplane2.gif" style="width:25.0%" /> <img
data-src="./mug2.gif" style="width:25.0%" /></p>
</section>
<section id="references" class="slide level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-bouchard2007efficient" class="csl-entry" role="listitem">
Bouchard, Guillaume. 2007. <span>“Efficient Bounds for the Softmax
Function and Applications to Approximate Inference in Hybrid
Models.”</span> In <em>NIPS 2007 Workshop for Approximate Bayesian
Inference in Continuous/Hybrid Systems</em>. Vol. 6.
</div>
<div id="ref-calli2015ycb" class="csl-entry" role="listitem">
Calli, Berk, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter
Abbeel, and Aaron M Dollar. 2015. <span>“The Ycb Object and Model Set:
Towards Common Benchmarks for Manipulation Research.”</span> In <em>2015
International Conference on Advanced Robotics (ICAR)</em>, 510–17. IEEE.
</div>
<div id="ref-chang2015shapenet" class="csl-entry" role="listitem">
Chang, Angel X., Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan,
Qixing Huang, Zimo Li, Silvio Savarese, et al. 2015.
<span>“<span>ShapeNet: An Information-Rich 3D Model
Repository</span>.”</span> arXiv:1512.03012 [cs.GR]. Stanford University
— Princeton University — Toyota Technological Institute at Chicago.
</div>
<div id="ref-daunizeau2017semi" class="csl-entry" role="listitem">
Daunizeau, Jean. 2017. <span>“Semi-Analytical Approximations to
Statistical Moments of Sigmoid and Softmax Mappings of Normal
Variables.”</span> <em>arXiv Preprint arXiv:1703.00091</em>.
</div>
<div id="ref-deitke2023objaverse" class="csl-entry" role="listitem">
Deitke, Matt, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel,
Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and
Ali Farhadi. 2023. <span>“Objaverse: A Universe of Annotated 3d
Objects.”</span> In <em>Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition</em>, 13142–53.
</div>
<div id="ref-jaakkola1997variational" class="csl-entry" role="listitem">
Jaakkola, Tommi S, and Michael I Jordan. 1997. <span>“A Variational
Approach to Bayesian Logistic Regression Models and Their
Extensions.”</span> In <em>Sixth International Workshop on Artificial
Intelligence and Statistics</em>, 283–94. PMLR.
</div>
<div id="ref-ramos2016hilbert" class="csl-entry" role="listitem">
Ramos, Fabio, and Lionel Ott. 2016. <span>“Hilbert Maps: Scalable
Continuous Occupancy Mapping with Stochastic Gradient Descent.”</span>
<em>The International Journal of Robotics Research</em> 35 (14):
1717–30.
</div>
<div id="ref-senanayake2017bayesian" class="csl-entry" role="listitem">
Senanayake, Ransalu, and Fabio Ramos. 2017. <span>“Bayesian Hilbert Maps
for Dynamic Continuous Occupancy Mapping.”</span> In <em>Conference on
Robot Learning</em>, 458–71. PMLR.
</div>
<div id="ref-thomas2019learning" class="csl-entry" role="listitem">
Thomas, Hugues. 2019. <span>“Learning New Representations for 3D Point
Cloud Semantic Segmentation.”</span> PhD thesis, Universit<span>é</span>
Paris sciences et lettres.
</div>
<div id="ref-van2020learning" class="csl-entry" role="listitem">
Van der Merwe, Mark, Qingkai Lu, Balakumar Sundaralingam, Martin Matak,
and Tucker Hermans. 2020. <span>“Learning Continuous 3d Reconstructions
for Geometrically Aware Grasping.”</span> In <em>2020 IEEE International
Conference on Robotics and Automation (ICRA)</em>, 11516–22. IEEE.
</div>
<div id="ref-wright2024v" class="csl-entry" role="listitem">
Wright, Herbert, Weiming Zhi, Matthew Johnson-Roberson, and Tucker
Hermans. 2024. <span>“V-Prism: Probabilistic Mapping of Unknown Tabletop
Scenes.”</span> <em>arXiv Preprint arXiv:2403.08106</em>.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: true,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // reveal.js plugins
        plugins: [
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
